# warmup_project

## Drive Square
For this problem, I was supposed to figure out how to make my robot drive in a square. To do this, I thought that I should create a loop that repeated 4 times that made the robot advance and then turn. Once the loop was finished, I would reset the robot to be at rest. To set the movement and direction of the robot, I used the same concept as the twist functions used in the lab to modify and then publish velocity.

In terms of functions used, all I had to do outside of the initializing function was have an drive function that had the for loop (that made the square) since my robot did not need to interact with its environment and so didn't need other functions to complete the square. 


![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/ezgif.com-gif-maker.gif)

## Person Follower
For this problem, I had to figure out how get a robot to first track my position relative to it, and then have it move towards me. To do the first portion of this, I needed to use the LIDAR to get a full 360 degree scan around the robot up to a distance of 3 meters away, and then based off of the closest object to the robot, have it turn in that direction until I am directly in front of it. Once the robot points towards me, it should move straight towards me, adjusting its direction in the case of my movement or if it is slightly deviating off course for any reason. 

Outside of my initializing function, I put all of my code into the process_scan sequence. Within process_scan, I first used a for loop that ran 181 times, so that the variable i would go from 0 to 180, allowing me then to set two variables right and left, that could track the position of every object found on their half of the lidar (left tracked position from 0 to 180, while right tracked from 0 to -180). Within the for loop, I had two if statements to find which of the 360 data points tracked is the closest to the robot, and whether it is to the right or to the left. Based off of this, outside of the for loop, I put two other if statements that make the robot turn to the right or to the left depending of which direction it should turn towards the closest object, while turning at a proportional rate to the degree at which the object is found (turns faster when the object is behind then when it is closer to being in front). Finally we end with an if statement that makes the robot go forward if the closest object detected is approximately in front of it (-9 to 9 degrees). 

![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/persfol1.gif)
![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/persfol2.gif)


## Wall Follower
For this probelm, I needed my robot to move forwards until it found a wall. Once the wall is found, it should follow it forever. To find and keep track of the wall, I only used the left hand side of the robot (I only used the lidar from 0 to 180 degrees), meaning it should always follow a wall to the left of it (if we put the robot so that the wall was to the right of it, it would detect the wall before a collision and then correct itself). Like in person follower, we want to find the closest part of the wall to the robot, once it is in range. From there, we divide it in three parts: if the closest part of the wall is at a 90 degree LIDAR scan, then the robot is parallel and should not move, if it's less than 90 degrees, then the robot should turn right to either correct itself or to avoid a collision, and if it's greater than 90 degrees, then it should turn left to get closer to the wall since it is likely going away from it. We also want our robot to stay at around 25 cm away from the wall, so it will also turn to correct a distance that is either closer (turn right) or further (turn left) from 25 cm.

To implement this, after initializing, I put the bulk of the code in process_scan. There, I began by searching for the closest object within a 0.4 meter radius in the same manner as person follower, however, I also search for the closest object within a 1 meter radius as a backup. Once finding the closest part of the wall, we force the robot to default turn so that it approaches both the 90 degrees that mean it is parallel to the wall, and the 0.25 meters that mean it is at a constant distance, while having it move forward at a constant speed. We then use two special clauses. The first one is that if that closest part of the wall is behind the robot, then it will turn at a faster speed, since this most likely means the robot is potentially driving into open air, and if it doesn't turn fast enough, it will lose track of the wall. The second case, is that if no wall is detected within 0.4 meters, then the robot will just drive forward until it finds something, unless there is a wall within 1 meter, in which case it will turn towards the wall, heading towards it at an angle (this does not negatively affect the first approach to the wall, and is meant as a safety measure if the robot ever loses track of the wall for a moment). 

![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/wallfal1.gif)
![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/wallfal2.gif)
![Alt Text](https://github.com/m-lederman/warmup_project/blob/main/wallfal3.gif)

## Challenges
The biggest challenge I had was figuring out how the LIDAR worked, and then how to use it properly. What I mean by this, is in the case of person follower, I first tried to implement a code that would focus not on the closest object to it, but instead would break the moment it found an object, meaning that if something was 3 meters away but directly in front of it and something else was 1 meter away to the right, it would track the thing in front. This meant that the robot would just end up picking up random things and tracking those, unless I was the only the only thing in the room and made sure the robot stayed very far from the wall. Even then, after implementing what I thought was a better method, the robot still picks up random things at times, and takes longer than it should to hone in on me.

## Future work
If I had more time, I think I would have liked to see my robot have much smoother movement, since I felt like it was very clunky, especially in the case of person follower. The problem is this would have required a large overhaul of my code and I would have had to rethink my strategy of approaching the problem, which I felt just wasn't worth it. When talking to peers, they said they had their robots move and turn at the same time, which is something I did for wall follower, and so my robot's movements felt slightly cleaner, but I felt like my version was not optimized, and I didn't implement this is person follower since I was scared that it wouldn't have the proper opportunities to adjust. Admittedly, I'm not entirely sure how I would approach this problem to get the best results. 


## Takeaways 

The first and most important takeaway from this project was that I learned how to use the robot's basic functions. Using the LIDAR and having the robot move were two things I think I have gotten enough practice with through this project to get to the point where I am comfortable with their use. I now know and am used to using Twist and publishing these movements, in addition to how data.ranges works. These are functions of the robot that I assume I will have to keep using for the rest of the course, and I feel like if I hadn't had a project like this to drill the use of these functions into my head, I might have struggled, or at least had to spend a decent amount of time learning these functions during a different project.

The other important takeaway was that the last two parts of this project, through how I implemented LIDAR, forced me to visualize the plane like the robot would. Using data.ranges along a full or half circle around the robot to sense its surroundings, and then figuring out what I wanted the robot to focus on were things that I wouldn't have considered if I was purely thinking like a human, who can sense everything (except what is behind me). Thus, in that sense, I have gotten conditioned to viewing the world as a robot when needed . 
